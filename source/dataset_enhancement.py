# -*- coding: utf-8 -*-
"""
@Author  : Weimin Zhu
@Time    : 2021-09-28
@File    : dataset.py
"""

import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from random import Random
from collections import defaultdict

import torch
from torch_geometric.data import Data, InMemoryDataset
from torch_geometric.data import DataLoader
# from torch_geometric.loader import DataLoader

from rdkit import Chem
from rdkit.Chem.BRICS import FindBRICSBonds
from rdkit.Chem.Scaffolds import MurckoScaffold
from rdkit import RDLogger

from utils import get_task_names
from smiles_bert import SMILESTransformer

from data_enhancement import data_enhancement

RDLogger.DisableLog('rdApp.*')


# -------------------------------------
# attentive_fp fashion featurization
# -------------------------------------
def onehot_encoding(x, allowable_set):
    if x not in allowable_set:
        raise Exception("input {0} not in allowable set{1}:".format(
            x, allowable_set))
    return [x == s for s in allowable_set]


def onehot_encoding_unk(x, allowable_set):
    """Maps inputs not in the allowable set to the last element."""
    if x not in allowable_set:
        x = allowable_set[-1]
    return [x == s for s in allowable_set]


def atom_attr(mol, explicit_H=False, use_chirality=True, pharmaco=True, scaffold=True):
    if pharmaco:
        mol = tag_pharmacophore(mol)
    if scaffold:
        mol = tag_scaffold(mol)

    feat = []
    for i, atom in enumerate(mol.GetAtoms()):
        results = onehot_encoding_unk(
            atom.GetSymbol(),
            ['B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At', 'other'
             ]) + onehot_encoding_unk(atom.GetDegree(),
                                      [0, 1, 2, 3, 4, 5, 'other']) + \
                  [atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \
                  onehot_encoding_unk(atom.GetHybridization(), [
                      Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,
                      Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,
                      Chem.rdchem.HybridizationType.SP3D2, 'other'
                  ]) + [atom.GetIsAromatic()]
        if not explicit_H:
            results = results + onehot_encoding_unk(atom.GetTotalNumHs(),
                                                    [0, 1, 2, 3, 4])
        if use_chirality:
            try:
                results = results + onehot_encoding_unk(
                    atom.GetProp('_CIPCode'),
                    ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]
            # print(one_of_k_encoding_unk(atom.GetProp('_CIPCode'), ['R', 'S']) + [atom.HasProp('_ChiralityPossible')])
            except:
                results = results + [0, 0] + [atom.HasProp('_ChiralityPossible')]
        if pharmaco:
            results = results + [int(atom.GetProp('Hbond_donor'))] + [int(atom.GetProp('Hbond_acceptor'))] + \
                      [int(atom.GetProp('Basic'))] + [int(atom.GetProp('Acid'))] + \
                      [int(atom.GetProp('Halogen'))]
        if scaffold:
            results = results + [int(atom.GetProp('Scaffold'))]
        feat.append(results)

    return np.array(feat)


def bond_attr(mol, use_chirality=True):
    feat = []
    index = []
    n = mol.GetNumAtoms()
    for i in range(n):
        for j in range(n):
            if i != j:
                bond = mol.GetBondBetweenAtoms(i, j)
                if bond is not None:
                    bt = bond.GetBondType()
                    bond_feats = [
                        bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE,
                        bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC,
                        bond.GetIsConjugated(),
                        bond.IsInRing()
                    ]
                    if use_chirality:
                        bond_feats = bond_feats + onehot_encoding_unk(
                            str(bond.GetStereo()),
                            ["STEREONONE", "STEREOANY", "STEREOZ", "STEREOE"])
                    feat.append(bond_feats)
                    index.append([i, j])

    return np.array(index), np.array(feat)


def bond_break(mol):
    results = np.array(sorted(list(FindBRICSBonds(mol))), dtype=np.long)

    if results.size == 0:
        cluster_idx = []
        Chem.rdmolops.GetMolFrags(mol, asMols=True, frags=cluster_idx)
        fra_edge_index, fra_edge_attr = bond_attr(mol)

    else:
        bond_to_break = results[:, 0, :]
        bond_to_break = bond_to_break.tolist()
        with Chem.RWMol(mol) as rwmol:
            for i in bond_to_break:
                rwmol.RemoveBond(*i)
        rwmol = rwmol.GetMol()
        cluster_idx = []
        Chem.rdmolops.GetMolFrags(rwmol, asMols=True, sanitizeFrags=False, frags=cluster_idx)
        fra_edge_index, fra_edge_attr = bond_attr(rwmol)
        cluster_idx = torch.LongTensor(cluster_idx)

    return fra_edge_index, fra_edge_attr, cluster_idx


# ---------------------------------------------
# Scaffold and pharmacophore information utils
# ---------------------------------------------
# tag pharmoco features to each atom
fun_smarts = {
        'Hbond_donor': '[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]',
        'Hbond_acceptor': '[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),n&X2&H0&+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]',
        'Basic': '[#7;+,$([N;H2&+0][$([C,a]);!$([C,a](=O))]),$([N;H1&+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);!$([C,a](=O))]),$([N;H0&+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))]),$([n;X2;+0;-0])]',
        'Acid': '[C,S](=[O,S,P])-[O;H1,-1]',
        'Halogen': '[F,Cl,Br,I]'
        }
FunQuery = dict([(pharmaco, Chem.MolFromSmarts(s)) for (pharmaco, s) in fun_smarts.items()])


def tag_pharmacophore(mol):
    for fungrp, qmol in FunQuery.items():
        matches = mol.GetSubstructMatches(qmol)
        match_idxes = []
        for mat in matches:
            match_idxes.extend(mat)
        for i, atom in enumerate(mol.GetAtoms()):
            tag = '1' if i in match_idxes else '0'
            atom.SetProp(fungrp, tag)
    return mol


# tag scaffold information to each atom
def tag_scaffold(mol):
    core = MurckoScaffold.GetScaffoldForMol(mol)
    match_idxes = mol.GetSubstructMatch(core)
    for i, atom in enumerate(mol.GetAtoms()):
        tag = '1' if i in match_idxes else '0'
        atom.SetProp('Scaffold', tag)
    return mol


# ---------------------------------
# data and dataset
# ---------------------------------
class MolData(Data):
    def __init__(self, fra_edge_index=None, fra_edge_attr=None, cluster_index=None, **kwargs):
        super(MolData, self).__init__(**kwargs)
        self.cluster_index = cluster_index
        self.fra_edge_index = fra_edge_index
        self.fra_edge_attr = fra_edge_attr

    def __inc__(self, key, value, *args, **kwargs):
        if key == 'cluster_index':
            return int(self.cluster_index.max()) + 1
        else:
            return super().__inc__(key, value, *args, **kwargs)


class MolDataset(InMemoryDataset):

    def __init__(self, root, dataset, task_type, tasks, logger=None,
                 transform=None, pre_transform=None, pre_filter=None):

        self.tasks = tasks
        self.dataset = dataset
        self.task_type = task_type
        self.logger = logger

        super(MolDataset, self).__init__(root, transform, pre_transform, pre_filter)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return ['{}.csv'.format(self.dataset)]

    @property
    def processed_file_names(self):
        return ['{}.pt'.format(self.dataset)]

    def download(self):
        pass

    '''
     用于Fine-tuning Bert + HiGNN
    '''
    # def process(self):
    #     df = pd.read_csv(self.raw_paths[0])
    #     smilesList = df.smiles.values
    #     self.logger.info(f'number of all smiles: {len(smilesList)}')
    #     remained_smiles = []
    #     canonical_smiles_list = []
    #     for smiles in smilesList:
    #         try:
    #             canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))
    #             remained_smiles.append(smiles)
    #         except:
    #             self.logger.info(f'not successfully processed smiles: {smiles}')
    #             pass
    #     self.logger.info(f'number of successfully processed smiles: {len(remained_smiles)}')
    #
    #     df = df[df["smiles"].isin(remained_smiles)].reset_index()
    #     target = df[self.tasks].values
    #     smilesList = df.smiles.values
    #     data_list = []
    #
    #     print('*** 开始组装数据 ***')
    #     for i, smi in enumerate(tqdm(smilesList)):
    #
    #         mol = Chem.MolFromSmiles(smi)
    #         data = self.mol2graph(mol)
    #
    #         if data is not None:
    #             label = target[i]
    #             label[np.isnan(label)] = 666
    #             data.y = torch.LongTensor([label])
    #             if self.task_type == 'regression':
    #                 data.y = torch.FloatTensor([label])
    #             data_list.append(data)
    #
    #     if self.pre_filter is not None:
    #         data_list = [data for data in data_list if self.pre_filter(data)]
    #     if self.pre_transform is not None:
    #         data_list = [self.pre_transform(data) for data in data_list]
    #
    #     data, slices = self.collate(data_list)
    #     torch.save((data, slices), self.processed_paths[0])


    '''
     用于Freeze Bert + HiGNN
    '''
    def process(self):
        df = pd.read_csv(self.raw_paths[0])
        smilesList = df.smiles.values
        self.logger.info(f'number of all smiles: {len(smilesList)}')
        remained_smiles = []
        canonical_smiles_list = []
        for smiles in smilesList:
            try:
                canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))
                remained_smiles.append(smiles)
            except:
                self.logger.info(f'not successfully processed smiles: {smiles}')
                pass
        self.logger.info(f'number of successfully processed smiles: {len(remained_smiles)}')

        df = df[df["smiles"].isin(remained_smiles)].reset_index()
        target = df[self.tasks].values
        smilesList = df.smiles.values
        data_list = []

        '''
        预处理SMILES_vec
        '''
        smiles_vec = get_smiles_vec(smilesList)

        print('*** 开始组装数据 ***')
        for i, smi in enumerate(tqdm(smilesList)):

            mol = Chem.MolFromSmiles(smi)
            data = self.mol2graph(mol, smi)

            if data is not None:
                label = target[i]
                label[np.isnan(label)] = 2
                data.y = torch.LongTensor([label])
                # 预处理数据：加入smiles_vec
                data.smiles_vec = smiles_vec[i]
                if self.task_type == 'regression':
                    data.y = torch.FloatTensor([label])
                data_list.append(data)

        if self.pre_filter is not None:
            data_list = [data for data in data_list if self.pre_filter(data)]
        if self.pre_transform is not None:
            data_list = [self.pre_transform(data) for data in data_list]

        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

    ''' 用于origin HiGNN'''
    # def process(self):
    #     df = pd.read_csv(self.raw_paths[0])
    #     smilesList = df.smiles.values
    #     self.logger.info(f'number of all smiles: {len(smilesList)}')
    #     remained_smiles = []
    #     canonical_smiles_list = []
    #     for smiles in smilesList:
    #         try:
    #             canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))
    #             remained_smiles.append(smiles)
    #         except:
    #             self.logger.info(f'not successfully processed smiles: {smiles}')
    #             pass
    #     self.logger.info(f'number of successfully processed smiles: {len(remained_smiles)}')
    #
    #     df = df[df["smiles"].isin(remained_smiles)].reset_index()
    #     target = df[self.tasks].values
    #     smilesList = df.smiles.values
    #     data_list = []
    #
    #     for i, smi in enumerate(tqdm(smilesList)):
    #
    #         mol = Chem.MolFromSmiles(smi)
    #         data = self.mol2graph(mol)
    #
    #         if data is not None:
    #             label = target[i]
    #             label[np.isnan(label)] = 666
    #             data.y = torch.LongTensor([label])
    #             if self.task_type == 'regression':
    #                 data.y = torch.FloatTensor([label])
    #             data_list.append(data)
    #
    #     if self.pre_filter is not None:
    #         data_list = [data for data in data_list if self.pre_filter(data)]
    #     if self.pre_transform is not None:
    #         data_list = [self.pre_transform(data) for data in data_list]
    #
    #     data, slices = self.collate(data_list)
    #     torch.save((data, slices), self.processed_paths[0])


    def mol2graph(self, mol, smi):
        smiles = Chem.MolToSmiles(mol)
        if mol is None: return None
        node_attr = atom_attr(mol)
        edge_index, edge_attr = bond_attr(mol)
        fra_edge_index, fra_edge_attr, cluster_index = bond_break(mol)
        data = MolData(
            x=torch.FloatTensor(node_attr),
            edge_index=torch.LongTensor(edge_index).t(),
            edge_attr=torch.FloatTensor(edge_attr),
            fra_edge_index=torch.LongTensor(fra_edge_index).t(),
            fra_edge_attr=torch.FloatTensor(fra_edge_attr),
            cluster_index=torch.LongTensor(cluster_index),
            y=None,
            smiles=smi,
        )
        return data


# ---------------------------------
# load dataset
# ---------------------------------
def load_dataset_random(path, dataset, seed, task_type, tasks=None, random_num=None, logger=None):
    save_path = path + 'processed/train_valid_test_{}_seed_{}_enhance_{}.ckpt'.format(dataset, seed, random_num)
    if os.path.isfile(save_path):
        trn, val, test = torch.load(save_path)
        return trn, val, test

    # 读取原数据集
    df = pd.read_csv(path + 'raw/' + dataset + '.csv')
    # 数据集按8:1:1划分
    indices = list(range(len(df)))
    random = Random(seed)
    random.seed(seed)
    random.shuffle(indices)
    trn_size = int(0.8 * len(df))
    val_size = int(0.1 * len(df))
    test_size = int(0.1 * len(df))

    trn_id = indices[:trn_size]
    val_id = indices[trn_size:(trn_size+val_size)]
    test_id = indices[(trn_size+val_size):(trn_size+val_size+test_size)]

    trn_df = df.loc[trn_id]
    val_df = df.loc[val_id]
    test_df = df.loc[test_id]

    # 训练集数据扩增
    print("random_num:{}".format(random_num))
    trn_enhance_df = data_enhancement(trn_df, random_num)
    dataset_enhancement = pd.concat([trn_enhance_df, val_df, test_df],ignore_index=True)
    print('trn_enhance_size:{}'.format(len(trn_enhance_df)))
    print('val_size:{}'.format(len(val_df)))
    print('test_size:{}'.format(len(test_df)))


    # 保存扩增后的数据集
    output_path = path + 'raw/' + dataset + '_enhancement.csv'
    dataset_enhancement.to_csv(output_path, index=False)

    pyg_dataset = MolDataset(root=path, dataset=dataset+'_enhancement', task_type=task_type, tasks=tasks, logger=logger)
    # 不删除SMILES
    # del pyg_dataset.data.smiles

    data_indices = list(range(len(pyg_dataset)))
    trn_ids = data_indices[:len(trn_enhance_df)]
    val_ids = data_indices[len(trn_enhance_df):(len(trn_enhance_df)+len(val_df))]
    test_ids = data_indices[(len(trn_enhance_df)+len(val_df)):(len(trn_enhance_df)+len(val_df)+len(test_df))]



    trn, val, test = pyg_dataset[torch.LongTensor(trn_ids)], \
                     pyg_dataset[torch.LongTensor(val_ids)], \
                     pyg_dataset[torch.LongTensor(test_ids)]

    logger.info(f'Total smiles = {len(pyg_dataset):,} | '
                f'train smiles = {len(trn_enhance_df):,} | '
                f'val smiles = {len(val):,} | '
                f'test smiles = {len(test):,}')



    # 处理数据集中不同类别的样本数量不平衡的问题
    assert task_type == 'classification' or 'regression'
    if task_type == 'classification':
        weights = []
        for i in range(len(tasks)):
            validId = np.where((pyg_dataset.data.y[:, i] == 0) | (pyg_dataset.data.y[:, i] == 1))[0]
            pos_len = (pyg_dataset.data.y[:, i][validId].sum()).item()
            neg_len = len(pyg_dataset.data.y[:, i][validId]) - pos_len
            # 根据正例和负例的数量计算权重:如果负例比正例多，则正例的权重更高，反之亦然
            weights.append([(neg_len + pos_len) / neg_len, (neg_len + pos_len) / pos_len])
        trn.weights = weights

    else:
        trn.weights = None

    torch.save([trn, val, test], save_path)
    return load_dataset_random(path, dataset, seed, task_type, tasks, random_num, logger)


# anti-noise experiments for hiv dataset
def load_dataset_noise(path, dataset, seed, task_type, tasks, rate, logger=None):
    save_path = path + 'processed/train_valid_test_{}_seed_{}_noise_{}.ckpt'.format(dataset, seed, int(100*rate))
    if os.path.isfile(save_path):
        trn, val, test = torch.load(save_path)
        return trn, val, test
    pyg_dataset = MolDataset(root=path, dataset=dataset, task_type=task_type, tasks=tasks, logger=logger)
    # del pyg_dataset.data.smiles

    train_size = int(0.8 * len(pyg_dataset))
    val_size = int(0.1 * len(pyg_dataset))
    test_size = len(pyg_dataset) - train_size - val_size

    pyg_dataset, perm = pyg_dataset.shuffle(return_perm=True)
    trn_perm, val_perm = perm[:train_size], perm[train_size:(train_size + val_size)]
    trn_cutoff, val_cutoff = int(train_size * rate), int(val_size*rate)
    trn_noise_perm, val_noise_perm = trn_perm[:trn_cutoff], val_perm[:val_cutoff]
    noise_perm = torch.cat([trn_noise_perm, val_noise_perm])

    # add same rate noise to train set and val set(simply change the label)
    pyg_dataset.data.y[noise_perm] = 1 - pyg_dataset.data.y[noise_perm]

    trn, val, test = pyg_dataset[:train_size], \
                     pyg_dataset[train_size:(train_size + val_size)], \
                     pyg_dataset[(train_size + val_size):]

    logger.info(f'Total smiles = {len(pyg_dataset):,} | '
                f'train smiles = {train_size:,} | '
                f'val smiles = {val_size:,} | '
                f'test smiles = {test_size:,}')

    weights = []
    pos_len = (pyg_dataset.data.y.sum()).item()
    neg_len = len(pyg_dataset) - pos_len
    weights.append([(neg_len + pos_len) / neg_len, (neg_len + pos_len) / pos_len])
    trn.weights = weights
    logger.info(weights)

    torch.save([trn, val, test], save_path)
    return load_dataset_noise(path, dataset, seed, task_type, tasks, rate)




def load_dataset_scaffold(path, dataset, seed, task_type, tasks=None, random_num=None, logger=None):
    save_path = path + 'processed/train_valid_test_{}_seed_{}_scaffold_enhance_{}.ckpt'.format(dataset, seed, random_num)
    if os.path.isfile(save_path):
        trn, val, test = torch.load(save_path)
        return trn, val, test

    # 读取原始数据集
    # pyg_dataset = MolDataset(root=path, dataset=dataset, task_type=task_type, tasks=tasks, logger=logger)

    df = pd.read_csv(path + 'raw/' + dataset + '.csv')
    dataset_list = df['smiles'].tolist()
    # 使用 scaffold_split 划分数据集
    trn_id, val_id, test_id= scaffold_split(dataset_list, task_type=task_type, tasks=tasks,
                                                      seed=seed, logger=logger)

    # 获取划分后的子集
    trn_df = df.loc[trn_id]
    val_df = df.loc[val_id]
    test_df = df.loc[test_id]

    # 训练集数据增强
    print("random_num:{}".format(random_num))
    trn_enhance_df = data_enhancement(trn_df, random_num)
    dataset_enhancement = pd.concat([trn_enhance_df, val_df, test_df], ignore_index=True)
    print('trn_enhance_size:{}'.format(len(trn_enhance_df)))
    print('val_size:{}'.format(len(val_df)))
    print('test_size:{}'.format(len(test_df)))

    # 保存增强后的数据集
    output_path = path + 'raw/' + dataset + '_enhancement.csv'
    dataset_enhancement.to_csv(output_path, index=False)

    # 加载增强后的数据集
    pyg_dataset = MolDataset(root=path, dataset=dataset+'_enhancement', task_type=task_type, tasks=tasks, logger=logger)

    # 重新划分数据索引
    data_indices = list(range(len(pyg_dataset)))
    trn_ids = data_indices[:len(trn_enhance_df)]
    val_ids = data_indices[len(trn_enhance_df):(len(trn_enhance_df) + len(val_df))]
    test_ids = data_indices[(len(trn_enhance_df) + len(val_df)):(len(trn_enhance_df) + len(val_df) + len(test_df))]

    # 获取增强后的训练集、验证集和测试集
    trn, val, test = pyg_dataset[torch.LongTensor(trn_ids)], \
                     pyg_dataset[torch.LongTensor(val_ids)], \
                     pyg_dataset[torch.LongTensor(test_ids)]

    logger.info(f'Total smiles = {len(pyg_dataset):,} | '
                f'train smiles = {len(trn_enhance_df):,} | '
                f'val smiles = {len(val):,} | '
                f'test smiles = {len(test):,}')

    # 处理数据集中不同类别的样本数量不平衡的问题
    assert task_type == 'classification' or 'regression'
    if task_type == 'classification':
        weights = []
        for i in range(len(tasks)):
            validId = np.where((pyg_dataset.data.y[:, i] == 0) | (pyg_dataset.data.y[:, i] == 1))[0]
            pos_len = (pyg_dataset.data.y[:, i][validId].sum()).item()
            neg_len = len(pyg_dataset.data.y[:, i][validId]) - pos_len
            # 根据正例和负例的数量计算权重:如果负例比正例多，则正例的权重更高，反之亦然
            weights.append([(neg_len + pos_len) / neg_len, (neg_len + pos_len) / pos_len])
        trn.weights = weights

    else:
        trn.weights = None

    # 保存处理后的数据集
    torch.save([trn, val, test], save_path)
    return load_dataset_scaffold(path, dataset, seed, task_type, tasks, random_num, logger)

# def load_dataset_scaffold(path, dataset, seed, task_type, tasks=None, logger=None):
#     save_path = path + 'processed/train_valid_test_{}_seed_{}_scaffold.ckpt'.format(dataset, seed)
#     if os.path.isfile(save_path):
#         trn, val, test = torch.load(save_path)
#         return trn, val, test
#
#     pyg_dataset = MolDataset(root=path, dataset=dataset, task_type=task_type, tasks=tasks, logger=logger)
#
#     trn_id, val_id, test_id, weights = scaffold_split(pyg_dataset, task_type=task_type, tasks=tasks,
#                                                       seed=seed, logger=logger)
#     # del pyg_dataset.data.smiles
#     trn, val, test = pyg_dataset[torch.LongTensor(trn_id)], \
#                      pyg_dataset[torch.LongTensor(val_id)], \
#                      pyg_dataset[torch.LongTensor(test_id)]
#     trn.weights = weights
#
#     torch.save([trn, val, test], save_path)
#     return load_dataset_scaffold(path, dataset, seed, task_type, tasks)


# ---------------------------------------------
# Scaffold utils, copy from chemprop.
# ---------------------------------------------
def generate_scaffold(mol, include_chirality=False):
    """
    Computes the Bemis-Murcko scaffold for a SMILES string.
    :param mol: A SMILES or an RDKit molecule.
    :param include_chirality: Whether to include chirality in the computed scaffold..
    :return: The Bemis-Murcko scaffold for the molecule.
    """
    mol = Chem.MolFromSmiles(mol) if type(mol) == str else mol
    scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=include_chirality)

    return scaffold


def scaffold_to_smiles(smiles, use_indices=False):
    """
    Computes the scaffold for each SMILES and returns a mapping from scaffolds to sets of smiles (or indices).
    :param smiles: A list of SMILES or RDKit molecules.
    :param use_indices: Whether to map to the SMILES's index in :code:`mols` rather than
                        mapping to the smiles string itself. This is necessary if there are duplicate smiles.
    :return: A dictionary mapping each unique scaffold to all SMILES (or indices) which have that scaffold.
    """
    scaffolds = defaultdict(set)
    for i, smi in enumerate(smiles):
        scaffold = generate_scaffold(smi)
        if use_indices:
            scaffolds[scaffold].add(i)
        else:
            scaffolds[scaffold].add(smi)

    return scaffolds


def scaffold_split(pyg_dataset, task_type, tasks, sizes=(0.8, 0.1, 0.1), balanced=True, seed=1, logger=None):

    assert sum(sizes) == 1

    # Split
    logger.info('generating scaffold......')
    num = len(pyg_dataset)
    train_size, val_size, test_size = sizes[0] * num, sizes[1] * num, sizes[2] * num
    train_ids, val_ids, test_ids = [], [], []
    train_scaffold_count, val_scaffold_count, test_scaffold_count = 0, 0, 0

    # Map from scaffold to index in the data
    scaffold_to_indices = scaffold_to_smiles(pyg_dataset, use_indices=True)

    # Seed randomness
    random = Random(seed)

    if balanced:  # Put stuff that's bigger than half the val/test size into train, rest just order randomly
        index_sets = list(scaffold_to_indices.values())
        big_index_sets = []
        small_index_sets = []
        for index_set in index_sets:
            if len(index_set) > val_size / 2 or len(index_set) > test_size / 2:
                big_index_sets.append(index_set)
            else:
                small_index_sets.append(index_set)
        random.seed(seed)
        random.shuffle(big_index_sets)
        random.shuffle(small_index_sets)
        index_sets = big_index_sets + small_index_sets
    else:  # Sort from largest to smallest scaffold sets
        index_sets = sorted(list(scaffold_to_indices.values()),
                            key=lambda index_set: len(index_set),
                            reverse=True)

    for index_set in index_sets:
        if len(train_ids) + len(index_set) <= train_size:
            train_ids += index_set
            train_scaffold_count += 1
        elif len(val_ids) + len(index_set) <= val_size:
            val_ids += index_set
            val_scaffold_count += 1
        else:
            test_ids += index_set
            test_scaffold_count += 1

    logger.info(f'Total scaffolds = {len(scaffold_to_indices):,} | '
                 f'train scaffolds = {train_scaffold_count:,} | '
                 f'val scaffolds = {val_scaffold_count:,} | '
                 f'test scaffolds = {test_scaffold_count:,}')

    logger.info(f'Total smiles = {num:,} | '
                 f'train smiles = {len(train_ids):,} | '
                 f'val smiles = {len(val_ids):,} | '
                 f'test smiles = {len(test_ids):,}')

    assert len(train_ids) + len(val_ids) + len(test_ids) == len(pyg_dataset)



    return train_ids, val_ids, test_ids


# ---------------------------------
# build dataset and dataloader
# ---------------------------------
def build_dataset(cfg, logger):

    cfg.defrost()
    task_name = get_task_names(os.path.join(cfg.DATA.DATA_PATH, 'raw/{}.csv'.format(cfg.DATA.DATASET)))
    if cfg.DATA.TASK_TYPE == 'classification':
        out_dim = 2 * len(task_name)
    elif cfg.DATA.TASK_TYPE == 'regression':
        out_dim = len(task_name)
    else:
        raise Exception('Unknown task type')
    opts = ['DATA.TASK_NAME', task_name, 'MODEL.GNN.OUT_DIM', out_dim]
    cfg.defrost()
    cfg.merge_from_list(opts)
    cfg.freeze()

    if cfg.DATA.SPLIT_TYPE == 'random':
        train_dataset, valid_dataset, test_dataset = load_dataset_random(cfg.DATA.DATA_PATH,
                                                                         cfg.DATA.DATASET,
                                                                         cfg.SEED,
                                                                         cfg.DATA.TASK_TYPE,
                                                                         cfg.DATA.TASK_NAME,
                                                                         cfg.DATA.AUG_FACTOR,
                                                                         logger)

    elif cfg.DATA.SPLIT_TYPE == 'scaffold':
        train_dataset, valid_dataset, test_dataset = load_dataset_scaffold(cfg.DATA.DATA_PATH,
                                                                           cfg.DATA.DATASET,
                                                                           cfg.SEED,
                                                                           cfg.DATA.TASK_TYPE,
                                                                           cfg.DATA.TASK_NAME,
                                                                           cfg.DATA.AUG_FACTOR,
                                                                           logger)

    elif cfg.DATA.SPLIT_TYPE == 'noise':
        train_dataset, valid_dataset, test_dataset = load_dataset_noise(cfg.DATA.DATA_PATH,
                                                                        cfg.DATA.DATASET,
                                                                        cfg.SEED,
                                                                        cfg.DATA.TASK_TYPE,
                                                                        cfg.DATA.TASK_NAME,
                                                                        cfg.DATA.RATE,
                                                                        logger)

    else:
        raise Exception('Unknown dataset split type')

    return train_dataset, valid_dataset, test_dataset


def build_loader_enhancement(cfg, logger):
    train_dataset, valid_dataset, test_dataset = build_dataset(cfg, logger)
    train_dataloader = DataLoader(train_dataset, batch_size=cfg.DATA.BATCH_SIZE, shuffle=True)
    # train_dataloader = DataLoader(train_dataset, batch_size=cfg.DATA.BATCH_SIZE, shuffle=False)
    valid_dataloader = DataLoader(valid_dataset, batch_size=cfg.DATA.BATCH_SIZE)
    test_dataloader = DataLoader(test_dataset, batch_size=cfg.DATA.BATCH_SIZE)
    weights = train_dataset.weights

    return train_dataloader, valid_dataloader, test_dataloader, weights



def get_smiles_vec(smiles_list):
    # 创建Bert模型
    bert_model = SMILESTransformer(bert_model_name='bert-base-uncased')
    smiles_vec = []
    print('*** smiles_vec开始处理 ***')
    for smiles in tqdm(smiles_list):
        # 获取SMILES_vec
        vec = bert_model(smiles).detach().numpy().tolist()
        smiles_vec.append(vec)
    return smiles_vec






